{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joexu22/llama2-finetune/blob/main/Notebooks/FineTune_LLaMA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8csmYWzxoyV"
      },
      "source": [
        "# This notebook finetunes and uploads a finetuned model\n",
        "\n",
        "credits: https://twitter.com/Dorialexander/status/1681671177696161794"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpQ6e2kNquzU"
      },
      "outputs": [],
      "source": [
        "# git the github gist for llama training code\n",
        "# not enough of a scientist to fiddle with all the settings/parameters\n",
        "\n",
        "!rm -rf 4f344196c5509d97a2288837fe6d8d4a\n",
        "!git clone https://gist.github.com/joexu22/4f344196c5509d97a2288837fe6d8d4a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-Fu7m-xro0K"
      },
      "outputs": [],
      "source": [
        "# install dependences\n",
        "\n",
        "!pip install accelerate==0.21.0\n",
        "!pip install peft==0.4.0\n",
        "!pip install bitsandbytes==0.40.2\n",
        "!pip install transformers==4.30.2\n",
        "!pip install trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZKjluVnr2H4"
      },
      "outputs": [],
      "source": [
        "# move to script location\n",
        "\n",
        "%cd 4f344196c5509d97a2288837fe6d8d4a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLEYid1Hsl8f"
      },
      "outputs": [],
      "source": [
        "# login to HuggingFace\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38kHmAiZr7a6"
      },
      "outputs": [],
      "source": [
        "# run script\n",
        "\n",
        "!python finetune_llama_v2.py --dataset_name \"UrbanJoe/LlamaMaster\" --max_steps 500 --merge_and_push True --model_name \"meta-llama/Llama-2-7b-chat-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZmiOpFcxKt5"
      },
      "outputs": [],
      "source": [
        "# merge the model together\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"results/final_merged_checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Quo5Bjx6mg"
      },
      "outputs": [],
      "source": [
        "# test the model\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = \"cuda:0\"\n",
        "model = model.to(device)\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
        "\n",
        "# prepare the prompt\n",
        "questions = \"How do I impress an interviewer?\"\n",
        "text = f\"### Human: {questions} ### Assistant: Hey bud,\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# delete extra from training\n",
        "del inputs['token_type_ids']\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "  output = model.generate(**inputs, max_new_tokens=1000)\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEN37rEn2aS_"
      },
      "outputs": [],
      "source": [
        "# Upload Model to HuggingFace\n",
        "\n",
        "model.push_to_hub(\"llama2-true-llama-master-ultimate\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNdWAplKQ8iWuLteLGrCszm",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
