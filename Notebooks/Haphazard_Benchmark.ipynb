{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMON9A1/h2TeXIgSxKGAzoh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joexu22/llama2-finetune/blob/main/Notebooks/Haphazard_Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Haphazard Benchmarking\n",
        "\n",
        "Quick and Dirty similiarty comparison between source, base, and finetune\n",
        "\n",
        "Source - Dataset (think client data)\n",
        "Base - Llama2 7b\n",
        "Finetune - The finetuned Llama\n",
        "\n",
        "Credit to LLaMA Index for Underlying Method"
      ],
      "metadata": {
        "id": "sQJqSXWGwm_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VRFVRtURyMG"
      },
      "outputs": [],
      "source": [
        "# clone the repo with the data\n",
        "\n",
        "!git clone https://github.com/joexu22/llama2-finetune.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_source = \"/content/llama2-finetune/data_processing/answers.txt\"\n",
        "path_base =\"/content/llama2-finetune/data_processing/llama_outputs/llama-base-hf-response-7b.txt\"\n",
        "path_finetune = \"/content/llama2-finetune/data_processing/llama_outputs/llama-finetuned-output-ultimate.txt\"\n"
      ],
      "metadata": {
        "id": "lidmGCicSIij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jerryjliu/llama_index.git\n",
        "%cd llama_index\n",
        "!pip install -r /content/llama_index/requirements.txt"
      ],
      "metadata": {
        "id": "-bbNzmbvUg4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "mnsJrGNsWPPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "2_K26ArxWr-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import SemanticSimilarityEvaluator\n",
        "\n",
        "evaluator = SemanticSimilarityEvaluator()"
      ],
      "metadata": {
        "id": "wMW8BDmZXOAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read the text files in source, base, and fine tune. Read them line by line and have them operatable\n",
        "\n",
        "import io\n",
        "\n",
        "def read_file(path):\n",
        "    with io.open(path, 'r', encoding='utf-8') as f:\n",
        "        return f.read().splitlines()\n",
        "\n",
        "source_text = read_file(path_source)\n",
        "base_text = read_file(path_base)\n",
        "finetune_text = read_file(path_finetune)\n",
        "\n",
        "# quick sanity check\n",
        "print(source_text)\n",
        "print(base_text)\n",
        "print(finetune_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "NIkJwhnGnK2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the Assistant part from the base and finetune\n",
        "\n",
        "import re\n",
        "\n",
        "# Creating a reusable function to extract and format the Assistant's part from a given text\n",
        "def extract_and_format_assistant_text(given_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract the Assistant's part from the given text and format it by removing special characters like \\n and \\'.\n",
        "\n",
        "    Parameters:\n",
        "        given_text (str): The text containing the Assistant's part.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted Assistant's part.\n",
        "    \"\"\"\n",
        "    assistant_part = re.search('Assistant: (.+)', given_text)\n",
        "    if assistant_part:\n",
        "        assistant_text = assistant_part.group(1)\n",
        "        # Replacing special characters like \\n and \\'\n",
        "        formatted_text = assistant_text.replace(\"\\\\n\", \" \").replace(\"\\\\'\", \"'\")\n",
        "        return formatted_text\n",
        "    else:\n",
        "        return \"Assistant's text not found\"\n",
        "\n",
        "# Testing the function\n",
        "test_text = '''b'### Human: What are MSE and RMSE? ### Assistant: Hey bud, MSE and RMSE are two commonly used metrics in machine learning to evaluate the performance of a model. Here\\\\'s the lowdown on what they mean:\\\\nMSE (Mean Squared Error) is a measure of the average squared difference between the predicted values and the actual values. It\\\\'s calculated by taking the average of the squared differences between the predicted and actual values for all data points in the dataset. MSE is a popular choi...'''\n",
        "print(extract_and_format_assistant_text(test_text))\n",
        "\n",
        "#tests\n",
        "#print(extract_and_format_assistant_text(base_text[1]))\n",
        "#print(extract_and_format_assistant_text(finetune_text[1]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vTTG_9_n2yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# general framework for measure similiarity between llama responses\n",
        "\n",
        "source_line = source_text[1]\n",
        "base_line = extract_and_format_assistant_text(base_text[1])\n",
        "finetune_line = extract_and_format_assistant_text(finetune_text[1])\n",
        "\n",
        "print(source_line)\n",
        "print(base_line)\n",
        "print(finetune_line)\n",
        "print()\n",
        "\n",
        "# difference between source and base\n",
        "result = await evaluator.aevaluate(\n",
        "    response=source_line,\n",
        "    reference=base_line,\n",
        ")\n",
        "\n",
        "print(\"Source and Base\")\n",
        "print(result.score)\n",
        "\n",
        "print()\n",
        "\n",
        "# difference between source and finetune\n",
        "result = await evaluator.aevaluate(\n",
        "    response=source_line,\n",
        "    reference=finetune_line,\n",
        ")\n",
        "\n",
        "print(\"Source and Finetune\")\n",
        "print(result.score)"
      ],
      "metadata": {
        "id": "Oi3FXeKGs7o0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}